{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwnybKD3hZB7"
      },
      "source": [
        "# Busca em Strings - Estrutura de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0h-P2rRibk6"
      },
      "source": [
        "## KMP - Knuth-Morris-Pratt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-AZaDVhjYOo"
      },
      "outputs": [],
      "source": [
        "pattern = \"ABABCABCAB\"\n",
        "text = \"ABABDABACDABABCABCABCABCABC\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xdv87nHZhXnR"
      },
      "outputs": [],
      "source": [
        "def build_lps_table(pattern):\n",
        "    m = len(pattern)\n",
        "    lps = [0] * m\n",
        "    length = 0\n",
        "    i = 1\n",
        "\n",
        "    while i < m:\n",
        "        if pattern[i] == pattern[length]:\n",
        "            length += 1\n",
        "            lps[i] = length\n",
        "            i += 1\n",
        "        else:\n",
        "            if length != 0:\n",
        "                length = lps[length - 1]\n",
        "            else:\n",
        "                lps[i] = 0\n",
        "                i += 1\n",
        "    return lps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhDDovK4hc1x"
      },
      "outputs": [],
      "source": [
        "def kmp_search(text, pattern):\n",
        "    n = len(text)\n",
        "    m = len(pattern)\n",
        "    lps = build_lps_table(pattern)\n",
        "    # print(lps)\n",
        "    matches = []\n",
        "\n",
        "    i = j = 0\n",
        "    while i < n:\n",
        "        if text[i] == pattern[j]:\n",
        "            i += 1\n",
        "            j += 1\n",
        "\n",
        "        if j == m:\n",
        "            matches.append(i - j)\n",
        "            j = lps[j - 1]\n",
        "        elif i < n and text[i] != pattern[j]:\n",
        "            if j != 0:\n",
        "                j = lps[j - 1]\n",
        "            else:\n",
        "                i += 1\n",
        "\n",
        "    return matches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L4Qxj-GhggA",
        "outputId": "ade73b8a-5e27-4b04-d5cc-10c9f39da418"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[10]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kmp_search(text, pattern)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_nwTsCRisqr"
      },
      "source": [
        "## Rabin-Karp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2yBqD0ahxE_"
      },
      "outputs": [],
      "source": [
        "def rabin_karp(text, pattern):\n",
        "    n, m = len(text), len(pattern)\n",
        "    d = 256  # tamanho do alfabeto\n",
        "    q = 101  # número primo\n",
        "\n",
        "    pattern_hash = text_hash = 0\n",
        "    h = pow(d, m-1) % q\n",
        "\n",
        "    # Calcula hash inicial\n",
        "    for i in range(m):\n",
        "        pattern_hash = (d * pattern_hash + ord(pattern[i])) % q\n",
        "        text_hash = (d * text_hash + ord(text[i])) % q\n",
        "\n",
        "    matches = []\n",
        "    for i in range(n - m + 1):\n",
        "        if pattern_hash == text_hash:\n",
        "            # Verificação caractere por caractere\n",
        "            if text[i:i+m] == pattern:\n",
        "                matches.append(i)\n",
        "\n",
        "        if i < n - m:\n",
        "            text_hash = (d * (text_hash - ord(text[i]) * h) + ord(text[i + m])) % q\n",
        "            if text_hash < 0:\n",
        "                text_hash += q\n",
        "\n",
        "    return matches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x15JcFPbjRz5",
        "outputId": "187c6b3e-f77b-4cf1-eef0-e72eb5ddbe9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[10]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rabin_karp(text, pattern)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQnnOvpljkId"
      },
      "source": [
        "## Aho-Corasick"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0AdyTUxqZk1",
        "outputId": "364a9b31-5418-4ea8-ad04-6f5764a71f17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exemplo 1 - Busca básica:\n",
            "Texto: 'ushers'\n",
            "Padrões: ['he', 'she', 'his', 'hers']\n",
            "Resultados:\n",
            "  'he' encontrado em posição 2-4\n",
            "  'she' encontrado em posição 1-4\n",
            "  'hers' encontrado em posição 2-6\n",
            "\n",
            "Exemplo 2 - Busca com contexto:\n",
            "Texto: 'The quick brown fox jumps over the lazy dog. She sells seashells by the seashore.'\n",
            "Padrões: ['the', 'she', 'sells']\n",
            "\n",
            "Padrão 'the':\n",
            "  Posição 0-3: 'The'\n",
            "  Contexto: 'The quick bro'\n",
            "  Posição 31-34: 'the'\n",
            "  Contexto: 'umps over the lazy dog.'\n",
            "  Posição 68-71: 'the'\n",
            "  Contexto: 'shells by the seashore.'\n",
            "\n",
            "Padrão 'she':\n",
            "  Posição 45-48: 'She'\n",
            "  Contexto: 'lazy dog. She sells sea'\n",
            "  Posição 58-61: 'she'\n",
            "  Contexto: ' sells seashells by the'\n",
            "\n",
            "Padrão 'sells':\n",
            "  Posição 49-54: 'sells'\n",
            "  Contexto: ' dog. She sells seashells'\n",
            "\n",
            "Estatísticas do automato:\n",
            "  num_patterns: 3\n",
            "  num_states: 11\n",
            "  total_pattern_length: 11\n",
            "  patterns: ['the', 'she', 'sells']\n",
            "  case_sensitive: False\n",
            "  only_whole_words: False\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict, deque\n",
        "from typing import List, Dict, Tuple, Optional, Set\n",
        "import re\n",
        "\n",
        "\n",
        "class AhoCorasick:\n",
        "\n",
        "    def __init__(self,\n",
        "                 patterns: List[str],\n",
        "                 case_sensitive: bool = True,\n",
        "                 only_whole_words: bool = False):\n",
        "        \"\"\"\n",
        "        Inicializa o automato Aho-Corasick.\n",
        "\n",
        "        Args:\n",
        "            patterns: Lista de padrões para buscar\n",
        "            case_sensitive: Se True, busca é sensível a maiúsculas/minúsculas\n",
        "            only_whole_words: Se True, busca apenas palavras completas\n",
        "        \"\"\"\n",
        "        self.case_sensitive = case_sensitive\n",
        "        self.only_whole_words = only_whole_words\n",
        "\n",
        "        # Validação e preprocessamento dos padrões\n",
        "        self.patterns = self._validate_and_preprocess_patterns(patterns)\n",
        "\n",
        "        if not self.patterns:\n",
        "            raise ValueError(\"Nenhum padrão válido fornecido\")\n",
        "\n",
        "        # Estruturas do automato\n",
        "        self.goto = {}  # Função de transição (usando dict para economia de memória)\n",
        "        self.fail = {}  # Função de falha\n",
        "        self.output = defaultdict(set)  # Função de saída\n",
        "\n",
        "        # Construção do automato\n",
        "        self._build_goto_function()\n",
        "        self._build_failure_function()\n",
        "\n",
        "        # Regex para validação de palavras completas\n",
        "        if self.only_whole_words:\n",
        "            self.word_boundary = re.compile(r'\\b')\n",
        "\n",
        "    def _validate_and_preprocess_patterns(self, patterns: List[str]) -> List[str]:\n",
        "        \"\"\"Valida e preprocessa os padrões de entrada.\"\"\"\n",
        "        if not patterns:\n",
        "            return []\n",
        "\n",
        "        processed = []\n",
        "        seen = set()\n",
        "\n",
        "        for pattern in patterns:\n",
        "            if not isinstance(pattern, str):\n",
        "                continue\n",
        "\n",
        "            # Preprocessamento baseado nas configurações\n",
        "            if not self.case_sensitive:\n",
        "                pattern = pattern.lower()\n",
        "\n",
        "            # Remove duplicatas\n",
        "            if pattern not in seen:\n",
        "                processed.append(pattern)\n",
        "                seen.add(pattern)\n",
        "\n",
        "        return processed\n",
        "\n",
        "    def _build_goto_function(self):\n",
        "        \"\"\"Constrói a função goto (trie).\"\"\"\n",
        "        # Estado inicial\n",
        "        self.goto[0] = {}\n",
        "        state_count = 1\n",
        "\n",
        "        # Constrói a trie\n",
        "        for pattern_idx, pattern in enumerate(self.patterns):\n",
        "            current_state = 0\n",
        "\n",
        "            for char in pattern:\n",
        "                if char not in self.goto[current_state]:\n",
        "                    self.goto[current_state][char] = state_count\n",
        "                    self.goto[state_count] = {}\n",
        "                    state_count += 1\n",
        "\n",
        "                current_state = self.goto[current_state][char]\n",
        "\n",
        "            # Marca o estado final\n",
        "            self.output[current_state].add(pattern_idx)\n",
        "\n",
        "    def _build_failure_function(self):\n",
        "        \"\"\"Constrói a função de falha usando BFS.\"\"\"\n",
        "        # Inicializa a função de falha\n",
        "        self.fail[0] = 0\n",
        "\n",
        "        # Fila para BFS\n",
        "        queue = deque()\n",
        "\n",
        "        # Estados de profundidade 1 têm falha para o estado 0\n",
        "        for char, state in self.goto[0].items():\n",
        "            self.fail[state] = 0\n",
        "            queue.append(state)\n",
        "\n",
        "        # Processa os demais estados\n",
        "        while queue:\n",
        "            current_state = queue.popleft()\n",
        "\n",
        "            for char, next_state in self.goto[current_state].items():\n",
        "                queue.append(next_state)\n",
        "\n",
        "                # Encontra o estado de falha\n",
        "                failure_state = self.fail[current_state]\n",
        "\n",
        "                while failure_state != 0 and char not in self.goto[failure_state]:\n",
        "                    failure_state = self.fail[failure_state]\n",
        "\n",
        "                if char in self.goto[failure_state]:\n",
        "                    failure_state = self.goto[failure_state][char]\n",
        "\n",
        "                self.fail[next_state] = failure_state\n",
        "\n",
        "                # Combina as saídas\n",
        "                self.output[next_state].update(self.output[failure_state])\n",
        "\n",
        "    def search(self, text: str) -> Dict[str, List[Tuple[int, int]]]:\n",
        "        \"\"\"\n",
        "        Busca todos os padrões no texto.\n",
        "\n",
        "        Args:\n",
        "            text: Texto onde buscar\n",
        "\n",
        "        Returns:\n",
        "            Dicionário com padrão -> lista de (início, fim) das ocorrências\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return {}\n",
        "\n",
        "        # Preprocessa o texto\n",
        "        search_text = text if self.case_sensitive else text.lower()\n",
        "\n",
        "        results = defaultdict(list)\n",
        "        current_state = 0\n",
        "\n",
        "        for i, char in enumerate(search_text):\n",
        "            # Encontra o próximo estado\n",
        "            while current_state != 0 and char not in self.goto[current_state]:\n",
        "                current_state = self.fail[current_state]\n",
        "\n",
        "            if char in self.goto[current_state]:\n",
        "                current_state = self.goto[current_state][char]\n",
        "\n",
        "            # Verifica se há padrões que terminam neste estado\n",
        "            if current_state in self.output:\n",
        "                for pattern_idx in self.output[current_state]:\n",
        "                    pattern = self.patterns[pattern_idx]\n",
        "                    start_pos = i - len(pattern) + 1\n",
        "                    end_pos = i + 1\n",
        "\n",
        "                    # Verifica se é palavra completa (se necessário)\n",
        "                    if self.only_whole_words:\n",
        "                        if not self._is_whole_word(text, start_pos, end_pos):\n",
        "                            continue\n",
        "\n",
        "                    results[pattern].append((start_pos, end_pos))\n",
        "\n",
        "        return dict(results)\n",
        "\n",
        "    def _is_whole_word(self, text: str, start: int, end: int) -> bool:\n",
        "        \"\"\"Verifica se a ocorrência é uma palavra completa.\"\"\"\n",
        "        # Verifica o caractere anterior\n",
        "        if start > 0 and text[start - 1].isalnum():\n",
        "            return False\n",
        "\n",
        "        # Verifica o caractere posterior\n",
        "        if end < len(text) and text[end].isalnum():\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def search_with_context(self,\n",
        "                          text: str,\n",
        "                          context_length: int = 20) -> Dict[str, List[Dict]]:\n",
        "        \"\"\"\n",
        "        Busca padrões retornando contexto adicional.\n",
        "\n",
        "        Args:\n",
        "            text: Texto onde buscar\n",
        "            context_length: Número de caracteres de contexto antes/depois\n",
        "\n",
        "        Returns:\n",
        "            Dicionário com informações detalhadas das ocorrências\n",
        "        \"\"\"\n",
        "        results = self.search(text)\n",
        "        detailed_results = defaultdict(list)\n",
        "\n",
        "        for pattern, positions in results.items():\n",
        "            for start, end in positions:\n",
        "                context_start = max(0, start - context_length)\n",
        "                context_end = min(len(text), end + context_length)\n",
        "\n",
        "                detailed_results[pattern].append({\n",
        "                    'start': start,\n",
        "                    'end': end,\n",
        "                    'match': text[start:end],\n",
        "                    'context': text[context_start:context_end],\n",
        "                    'context_start': context_start,\n",
        "                    'context_end': context_end\n",
        "                })\n",
        "\n",
        "        return dict(detailed_results)\n",
        "\n",
        "    def count_occurrences(self, text: str) -> Dict[str, int]:\n",
        "        \"\"\"Conta o número de ocorrências de cada padrão.\"\"\"\n",
        "        results = self.search(text)\n",
        "        return {pattern: len(positions) for pattern, positions in results.items()}\n",
        "\n",
        "    def get_statistics(self) -> Dict:\n",
        "        \"\"\"Retorna estatísticas sobre o automato construído.\"\"\"\n",
        "        return {\n",
        "            'num_patterns': len(self.patterns),\n",
        "            'num_states': len(self.goto),\n",
        "            'total_pattern_length': sum(len(p) for p in self.patterns),\n",
        "            'patterns': self.patterns.copy(),\n",
        "            'case_sensitive': self.case_sensitive,\n",
        "            'only_whole_words': self.only_whole_words\n",
        "        }\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # print(\"=== Demonstração do Aho-Corasick ===\\n\")\n",
        "\n",
        "    # Exemplo 1: Busca básica\n",
        "    patterns = [\"he\", \"she\", \"his\", \"hers\"]\n",
        "    text = \"ushers\"\n",
        "\n",
        "    ac = AhoCorasick(patterns, case_sensitive=False)\n",
        "    results = ac.search(text)\n",
        "\n",
        "    print(\"Exemplo 1 - Busca básica:\")\n",
        "    print(f\"Texto: '{text}'\")\n",
        "    print(f\"Padrões: {patterns}\")\n",
        "    print(\"Resultados:\")\n",
        "    for pattern, positions in results.items():\n",
        "        for start, end in positions:\n",
        "            print(f\"  '{pattern}' encontrado em posição {start}-{end}\")\n",
        "\n",
        "    # Exemplo 2: Busca com contexto\n",
        "    print(\"\\nExemplo 2 - Busca com contexto:\")\n",
        "    long_text = \"The quick brown fox jumps over the lazy dog. She sells seashells by the seashore.\"\n",
        "    patterns2 = [\"the\", \"she\", \"sells\"]\n",
        "\n",
        "    ac2 = AhoCorasick(patterns2, case_sensitive=False)\n",
        "    detailed_results = ac2.search_with_context(long_text, context_length=10)\n",
        "\n",
        "    print(f\"Texto: '{long_text}'\")\n",
        "    print(f\"Padrões: {patterns2}\")\n",
        "    for pattern, matches in detailed_results.items():\n",
        "        print(f\"\\nPadrão '{pattern}':\")\n",
        "        for _match in matches:\n",
        "            print(f\"  Posição {_match['start']}-{_match['end']}: '{_match['match']}'\")\n",
        "            print(f\"  Contexto: '{_match['context']}'\")\n",
        "\n",
        "    # Exemplo 3: Estatísticas\n",
        "    print(\"\\nEstatísticas do automato:\")\n",
        "    stats = ac2.get_statistics()\n",
        "    for key, value in stats.items():\n",
        "        print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_t6df5Vijd3"
      },
      "source": [
        "> Busca em Strings"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
